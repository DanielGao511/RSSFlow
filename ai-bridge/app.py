import os
import redis
import feedparser
from flask import Flask, request, Response
from openai import OpenAI
from dotenv import load_dotenv
import hashlib
import re
import concurrent.futures  # âœ… å¼•å…¥å¹¶å‘åº“
import socket
import urllib.parse

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
app = Flask(__name__)

# --- é…ç½® ---
ALIYUN_API_KEY = os.getenv("ALIYUN_API_KEY")
ALIYUN_BASE_URL = os.getenv("ALIYUN_BASE_URL")
REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))

# âœ… å…¨å±€ socket è¶…æ—¶è®¾ç½®ä¸º 120ç§’ï¼Œé˜²æ­¢ RSSHub æŠ“å–é•¿æ–‡æ—¶ AI-Bridge æå‰æ–­å¼€
socket.setdefaulttimeout(120)

# --- åˆå§‹åŒ– Redis ---
try:
    cache = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0, decode_responses=True)
    cache.ping()
    print("âœ… Redis è¿æ¥æˆåŠŸ")
except Exception as e:
    print(f"âš ï¸ Redis è¿æ¥å¤±è´¥: {e}")
    cache = None


def get_ai_processing(title, content):
    """
    è°ƒç”¨ AI è¿›è¡Œç¿»è¯‘å’Œæ€»ç»“çš„æ ¸å¿ƒå‡½æ•°
    """
    # 1. æ£€æŸ¥ç¼“å­˜
    if not cache:
        return title, "Redis Error (No Cache)", content

    content_hash = hashlib.md5((title + content[:200]).encode()).hexdigest()
    cache_key = f"ai_result_v17_aliyun:{content_hash}"

    cached = cache.get(cache_key)
    if cached:
        try:
            parts = cached.split("|||")
            if len(parts) == 2:
                return parts[0], parts[1]
        except:
            pass

    # 2. å‡†å¤‡ Prompt
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»ä¸»ç¼–ã€‚è¯·å°†æ–°é—»ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œå¹¶é‡æ„ä¸ºé€‚åˆé˜…è¯»çš„å¹²å‡€HTMLæ ¼å¼ã€‚"
    )
    user_prompt = f"""
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸­é—´ç”¨ ||| åˆ†éš”ä¸¤ä¸ªéƒ¨åˆ†ï¼š
    1. ä¸­æ–‡ç¿»è¯‘åçš„åŸæ–‡æ ‡é¢˜
    2. ä¸­æ–‡å…¨æ–‡ç¿»è¯‘ï¼ˆå¿…é¡»éµå®ˆä»¥ä¸‹HTMLæ¸…æ´—è§„åˆ™ï¼‰

    ã€HTMLæ¸…æ´—ä¸ç¿»è¯‘è§„åˆ™ã€‘ï¼š
    - **ä¸¥ç¦ä½¿ç”¨** <div, <span, <nav, <style> æ ‡ç­¾ã€‚
    - **ä¸¥ç¦ä¿ç•™** ä»»ä½• class="...", style="...", id="..." å±æ€§ã€‚
    - æ­£æ–‡æ®µè½å¿…é¡»ç”¨ <p> æ ‡ç­¾åŒ…è£¹ã€‚
    - å°æ ‡é¢˜ä½¿ç”¨ <h3> æˆ– <h4> æ ‡ç­¾ã€‚
    - ä»…ä¿ç•™ <img>, <p>, <b>, <strong>, <blockquote>, <ul>, <li>, <a> è¿™äº›åŸºç¡€æ ‡ç­¾ã€‚
    - ç¡®ä¿å›¾ç‰‡é“¾æ¥ <img> å®Œæ•´ä¿ç•™ã€‚

    åŸæ–‡æ ‡é¢˜ï¼š{title}
    åŸæ–‡å†…å®¹ï¼š{content} 
    """

    try:
        if not ALIYUN_API_KEY:
            raise ValueError("ALIYUN_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼")

        client = OpenAI(api_key=ALIYUN_API_KEY, base_url=ALIYUN_BASE_URL)

        completion = client.chat.completions.create(
            model=os.getenv("MODEL_NAME"),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.1,
            timeout=120,  # AI è¯·æ±‚æœ¬èº«çš„è¶…æ—¶
        )
        result = completion.choices[0].message.content.strip()

        # æ¸…æ´—
        result = result.replace("```html", "").replace("```", "")
        parts = result.split("|||")

        cn_title = parts[0].strip() if len(parts) > 0 else title
        cn_content = parts[1].strip() if len(parts) > 1 else content

        # å†™å…¥ç¼“å­˜
        cache.setex(cache_key, 604800, f"{cn_title}|||{cn_content}")
        return cn_title, cn_content

    except Exception as e:
        print(f"âŒ Aliyun Error processing {title[:10]}: {e}")
        return (
            title,
            "âš ï¸ AIæœåŠ¡å¼‚å¸¸",
            f"é”™è¯¯è¯¦æƒ…: {str(e)}<br><br>åŸå§‹å†…å®¹:<br>{content}",
        )


def extract_first_image(html_content):
    if not html_content:
        return None
    match = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', html_content)
    return match.group(1) if match else None


def generate_xml(entries, original_feed):
    xml = ['<?xml version="1.0" encoding="UTF-8" ?>']
    xml.append(
        '<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">'
    )
    xml.append("<channel>")
    feed_title = original_feed.feed.get("title", "Unknown Feed")
    xml.append(f"<title>qwen AI - {feed_title}</title>")

    for entry in entries:
        xml.append("<item>")
        xml.append(f'<title><![CDATA[{entry["cn_title"]}]]></title>')
        xml.append(f'<link>{entry["link"]}</link>')
        xml.append(f'<description><![CDATA[{entry["cn_content"]}]]></description>')
        xml.append(
            f'<content:encoded><![CDATA[{entry["cn_content"]}]]></content:encoded>'
        )
        xml.append(f'<guid>{entry.get("id", entry["link"])}</guid>')
        xml.append("</item>")

    xml.append("</channel></rss>")
    return "".join(xml)


def process_single_entry(args):
    """
    å·¥ä½œçº¿ç¨‹å‡½æ•°ï¼šå¤„ç†å•ä¸ªæ¡ç›®
    """
    index, entry = args

    title = entry.get("title", "æ— æ ‡é¢˜")
    link = entry.get("link", "")
    # ä¼˜å…ˆå– descriptionï¼Œæœ‰äº› RSS æº content åœ¨ summary é‡Œ
    raw_content = entry.get("description") or entry.get("summary") or ""

    # è°ƒç”¨ AI
    cn_title, cn_content = get_ai_processing(title, raw_content)

    img_url = extract_first_image(raw_content)
    img_tag = f'<img src="{img_url}"><br>' if img_url else ""

    return {
        "index": index,
        "cn_title": cn_title,
        "link": link,
        "cn_content": cn_content,
        "id": entry.get("id", link),
    }


@app.route("/feed")
def proxy_feed():
    target_url = request.args.get("url")
    if not target_url:
        return "Missing url", 400

    remaining_args = request.args.copy()
    remaining_args.pop("url")  # ç§»é™¤å·²ç»è·å–çš„ url å‚æ•°æœ¬èº«

    if remaining_args:
        # åˆ¤æ–­è¿æ¥ç¬¦ï¼šå¦‚æœ target_url åŸæœ¬å°±æœ‰ '?'ï¼Œåé¢å°±ç”¨ '&' æ‹¼æ¥ï¼Œå¦åˆ™ç”¨ '?'
        connector = "&" if "?" in target_url else "?"
        extra_params = []
        for key, value in remaining_args.items():
            extra_params.append(f"{key}={value}")

        target_url += f"{connector}{'&'.join(extra_params)}"
    print(f"ğŸ“¥ æ­£åœ¨æŠ“å–: {target_url}")  # ğŸ‘ˆ ç°åœ¨çœ‹æ—¥å¿—ï¼Œè¿™é‡Œåº”è¯¥ä¼šæœ‰ ?mode=fulltext äº†

    try:
        # è§£æ RSS
        feed = feedparser.parse(target_url)
        # ç®€å•çš„é”™è¯¯æ£€æŸ¥ (æ³¨æ„ï¼šæœ‰äº›æºè™½ç„¶æˆåŠŸä½† bozo ä¹Ÿæ˜¯ 1ï¼Œæ‰€ä»¥è¿™é‡Œåªåšè®°å½•ä¸å¼ºåˆ¶æŠ¥é”™)
        if not feed.entries and feed.bozo:
            print(f"âš ï¸ RSS Parse Warning: {feed.bozo_exception}")
    except Exception as e:
        return f"Error fetching feed: {str(e)}", 500

    target_entries = feed.entries
    if not target_entries:
        return Response(generate_xml([], feed), mimetype="application/xml")

    print(f"âš¡ å¼€å§‹å¹¶å‘å¤„ç† {len(target_entries)} æ¡å†…å®¹...")

    processed_results = []
    # å‡†å¤‡å¸¦ç´¢å¼•çš„ä»»åŠ¡ï¼Œç¡®ä¿æœ€åèƒ½æ’å›æ¥
    tasks = [(i, entry) for i, entry in enumerate(target_entries)]

    # === âœ… å¹¶å‘æ‰§è¡Œ ===
    # max_workers=5 æ„å‘³ç€ 5 ç¯‡æ–‡ç« åŒæ—¶è·‘ï¼Œé€Ÿåº¦æå‡ 5 å€
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_entry = {
            executor.submit(process_single_entry, task): task for task in tasks
        }

        for future in concurrent.futures.as_completed(future_to_entry):
            try:
                data = future.result()
                processed_results.append(data)
            except Exception as exc:
                print(f"Task generated an exception: {exc}")

    # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åº
    processed_results.sort(key=lambda x: x["index"])

    print(f"âœ… å¤„ç†å®Œæˆï¼Œè¿”å› XML")
    return Response(generate_xml(processed_results, feed), mimetype="application/xml")


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
